Processor: Intel(R) Xeon(R) CPU @ 2.30GHz
RAM: 124Gi
GPU:     Product Name                          : Tesla V100-SXM2-16GB
    Product Name                          : Tesla V100-SXM2-16GB
    Product Name                          : Tesla V100-SXM2-16GB
    Product Name                          : Tesla V100-SXM2-16GB
U_Net

Running with 1 GPUs
Epoch is [1/2], batch size 16
Training time: 145.18 seconds
Communication time: 24.6134 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 124.06 seconds
Communication time: 4.4928 seconds
-----------------

Running with 2 GPUs
Epoch is [1/2], batch size 16
Epoch is [1/2], batch size 16
Training time: 75.81 seconds
Communication time: 3.8926 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 75.81 seconds
Communication time: 3.9308 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 74.72 seconds
Communication time: 2.3177 seconds
-----------------
Training time: 74.72 seconds
Communication time: 2.3346 seconds
-----------------

Running with 3 GPUs
Epoch is [1/2], batch size 16
Epoch is [1/2], batch size 16
Epoch is [1/2], batch size 16
Training time: 60.42 seconds
Communication time: 3.2719 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 60.42 seconds
Communication time: 3.1952 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 60.42 seconds
Communication time: 3.2802 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 60.07 seconds
Communication time: 1.6215 seconds
-----------------
Training time: 60.07 seconds
Communication time: 1.6301 seconds
-----------------
Training time: 60.07 seconds
Communication time: 1.6354 seconds
-----------------

Running with 4 GPUs
Epoch is [1/2], batch size 16
Epoch is [1/2], batch size 16
Epoch is [1/2], batch size 16
Epoch is [1/2], batch size 16
Training time: 51.98 seconds
Communication time: 2.9865 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 51.98 seconds
Communication time: 2.9990 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 51.98 seconds
Communication time: 2.9995 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 51.98 seconds
Communication time: 3.0543 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 51.43 seconds
Communication time: 1.2561 seconds
-----------------
Training time: 51.43 seconds
Communication time: 1.3132 seconds
-----------------
Training time: 51.42 seconds
Communication time: 1.2587 seconds
-----------------
Training time: 51.43 seconds
Communication time: 1.2980 seconds
-----------------
U_Net

Running with 1 GPUs
Epoch is [1/2], batch size 32
Traceback (most recent call last):
  File "/home/sz3714/HPML_Project/unnet/unet_opt8.py", line 385, in <module>
    mp.spawn(train, args=(world_size, net, batch_size, epochs, Load_train), nprocs=world_size, join=True)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/sz3714/HPML_Project/unnet/unet_opt8.py", line 331, in train
    out = net(img_data)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1040, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1000, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sz3714/HPML_Project/unnet/unet_opt8.py", line 218, in forward
    d2 = self.Up_conv2(d2)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sz3714/HPML_Project/unnet/unet_opt8.py", line 130, in forward
    x = self.conv(x)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 15.78 GiB total capacity; 12.75 GiB already allocated; 468.19 MiB free; 14.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

