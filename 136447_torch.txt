Processor: Intel(R) Xeon(R) CPU @ 2.30GHz
RAM: 124Gi
GPU:     Product Name                          : Tesla V100-SXM2-16GB
    Product Name                          : Tesla V100-SXM2-16GB
    Product Name                          : Tesla V100-SXM2-16GB
    Product Name                          : Tesla V100-SXM2-16GB
U_Net

Running with 1 GPUs
Epoch is [1/2], batch size 16
Training time: 131.34 seconds
Communication time: 21.1385 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 114.14 seconds
Communication time: 4.4703 seconds
-----------------

Running with 2 GPUs
Epoch is [1/2], batch size 16
Epoch is [1/2], batch size 16
Training time: 65.05 seconds
Communication time: 3.8248 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 65.05 seconds
Communication time: 3.8549 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 63.87 seconds
Communication time: 2.3028 seconds
-----------------
Training time: 63.87 seconds
Communication time: 2.3356 seconds
-----------------

Running with 3 GPUs
Epoch is [1/2], batch size 16
Epoch is [1/2], batch size 16
Epoch is [1/2], batch size 16
Training time: 49.55 seconds
Communication time: 3.2426 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 49.55 seconds
Communication time: 3.2595 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 49.55 seconds
Communication time: 3.2882 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 48.21 seconds
Communication time: 1.6505 seconds
-----------------
Training time: 48.21 seconds
Communication time: 1.5997 seconds
-----------------
Training time: 48.21 seconds
Communication time: 1.5972 seconds
-----------------

Running with 4 GPUs
Epoch is [1/2], batch size 16
Epoch is [1/2], batch size 16
Epoch is [1/2], batch size 16
Epoch is [1/2], batch size 16
Training time: 40.21 seconds
Communication time: 2.9763 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 40.21 seconds
Communication time: 3.0301 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 40.21 seconds
Communication time: 2.9148 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 40.21 seconds
Communication time: 2.9427 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 39.34 seconds
Communication time: 1.3024 seconds
-----------------
Training time: 39.34 seconds
Communication time: 1.2751 seconds
-----------------
Training time: 39.35 seconds
Communication time: 1.2972 seconds
-----------------
Training time: 39.35 seconds
Communication time: 1.2754 seconds
-----------------
U_Net

Running with 1 GPUs
Epoch is [1/2], batch size 32
Traceback (most recent call last):
  File "/home/sz3714/HPML_Project/unnet/unet_opt4.py", line 385, in <module>
    mp.spawn(train, args=(world_size, net, batch_size, epochs, Load_train), nprocs=world_size, join=True)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/sz3714/HPML_Project/unnet/unet_opt4.py", line 331, in train
    out = net(img_data)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1040, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1000, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sz3714/HPML_Project/unnet/unet_opt4.py", line 218, in forward
    d2 = self.Up_conv2(d2)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sz3714/HPML_Project/unnet/unet_opt4.py", line 130, in forward
    x = self.conv(x)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 15.78 GiB total capacity; 12.75 GiB already allocated; 468.19 MiB free; 14.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

