Processor: Intel(R) Xeon(R) CPU @ 2.30GHz
RAM: 124Gi
GPU:     Product Name                          : Tesla V100-SXM2-16GB
    Product Name                          : Tesla V100-SXM2-16GB
    Product Name                          : Tesla V100-SXM2-16GB
    Product Name                          : Tesla V100-SXM2-16GB
U_Net

Running with 1 GPUs
Epoch is [1/2], batch size 16
Training time: 128.33 seconds
Communication time: 21.0001 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 111.32 seconds
Communication time: 4.7241 seconds
-----------------

Running with 2 GPUs
Epoch is [1/2], batch size 16
Epoch is [1/2], batch size 16
Training time: 60.57 seconds
Communication time: 4.0122 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 60.57 seconds
Communication time: 3.9756 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 59.50 seconds
Communication time: 2.3902 seconds
-----------------
Training time: 59.50 seconds
Communication time: 2.3955 seconds
-----------------

Running with 3 GPUs
Epoch is [1/2], batch size 16
Epoch is [1/2], batch size 16
Epoch is [1/2], batch size 16
Training time: 43.61 seconds
Communication time: 3.2880 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 43.61 seconds
Communication time: 3.3214 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 43.62 seconds
Communication time: 3.3378 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 42.45 seconds
Communication time: 1.6845 seconds
-----------------
Training time: 42.45 seconds
Communication time: 1.6601 seconds
-----------------
Training time: 42.44 seconds
Communication time: 1.6657 seconds
-----------------

Running with 4 GPUs
Epoch is [1/2], batch size 16
Epoch is [1/2], batch size 16
Epoch is [1/2], batch size 16
Epoch is [1/2], batch size 16
Training time: 35.23 seconds
Communication time: 3.0282 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 35.23 seconds
Communication time: 3.0424 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 35.23 seconds
Communication time: 3.1086 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 35.23 seconds
Communication time: 3.0523 seconds
-----------------
Epoch is [2/2], batch size 16
Training time: 34.15 seconds
Communication time: 1.3076 seconds
-----------------
Training time: 34.14 seconds
Communication time: 1.3030 seconds
-----------------
Training time: 34.14 seconds
Communication time: 1.3428 seconds
-----------------
Training time: 34.14 seconds
Communication time: 1.3092 seconds
-----------------
U_Net

Running with 1 GPUs
Epoch is [1/2], batch size 32
Traceback (most recent call last):
  File "/home/sz3714/HPML_Project/unnet/unet_opt2.py", line 385, in <module>
    mp.spawn(train, args=(world_size, net, batch_size, epochs, Load_train), nprocs=world_size, join=True)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/sz3714/HPML_Project/unnet/unet_opt2.py", line 331, in train
    out = net(img_data)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1040, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1000, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sz3714/HPML_Project/unnet/unet_opt2.py", line 218, in forward
    d2 = self.Up_conv2(d2)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sz3714/HPML_Project/unnet/unet_opt2.py", line 130, in forward
    x = self.conv(x)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/ext3/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 15.78 GiB total capacity; 12.75 GiB already allocated; 468.19 MiB free; 14.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

